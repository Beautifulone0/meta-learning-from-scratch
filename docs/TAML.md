# TAML (Task Agnostic Meta Learning) - Implementation Overview

## ðŸ’¡ Intuition: Why Task-Agnostic Meta-Learning?

> **Prerequisite:** For best understanding, read [LEO.md](LEO.md) first. LEO and TAML both address meta-learning, but TAML takes a different approach: it aims to learn *how to adapt* without relying on task-specific signals.

### The Core Problem: Bias in Meta-Learning

Most meta-learning algorithms (like MAML, Meta-SGD, LEO) adapt model parameters for each task using gradients from the support set. However, these methods can become **biased** toward the specific tasks seen during meta-training. This bias can hurt generalization to new, unseen tasks.

**TAML's goal:** Prevent the model from overfitting to the meta-training tasks by learning *task-agnostic* initializations and adaptation mechanisms.

### Intuitive Analogy: Learning to Learn Without Overfitting

Imagine you're training for a decathlon, but you only ever practice running and jumping. If your coach only adapts your training for those two events, you'll struggle with swimming or cycling. TAML is like a coach who ensures your training is **balanced and general**, so you can adapt quickly to *any* eventâ€”even those you haven't seen before.

### How TAML Works

TAML introduces mechanisms to:
- **Discourage task-specific bias**: By regularizing the meta-learning process, TAML prevents the model from becoming too specialized.
- **Promote general adaptation**: The model learns initializations and adaptation rules that work well across a wide range of tasks.

TAML achieves this by:
- Using a shared feature extractor (CNN backbone)
- Learning to modulate and adapt only the final layers (head) for each task
- Employing an encoder and relation network to process support examples in a task-agnostic way
- Decoding latent representations into modulations and learning rates for adaptation

---

## Architecture Details

### TAMLClassifier
- **Shared CNN feature extractor**: Four Conv2d layers + BatchNorm + ReLU + MaxPool, followed by flattening
- **Task-specific head**: Two Linear layers (128 hidden units, then output to `num_classes`)
- **Forward pass**: Can use either the default head or explicit fast weights (adapted parameters)

### TAMLEncoder
- **Input**: Features from CNN (shape: `[batch, 2304]`)
- **Output**: Latent code (shape: `[batch, latent_dim]`)
- **Architecture**: Two Linear layers (512 hidden units, then output to `latent_dim`)

### TAMLRelationNetwork
- **Purpose**: Refines class latent codes by comparing each class to all others in the task
- **Architecture**: Two Linear layers (input: concatenated latent codes, output: refined latent code)

### TAMLDecoder
- **Purpose**: Decodes latent codes into modulations (scaling factors for head weights/biases) and learning rates
- **Approaches**:
  - `global_scalars`: One modulation and learning rate for all head parameters
  - `per_layer_scalars`: Separate modulations and learning rates for each head layer
- **Output**: Dicts of modulations and learning rates for head weights/biases

### TaskAgnosticMetaLearning (TAML)
- **Meta-learning loop**:
  1. Encode support set to latent codes
  2. Refine codes using relation network
  3. Aggregate codes and decode to modulations/learning rates
  4. Apply modulations to head parameters
  5. Inner loop: Adapt fast weights using decoded learning rates
  6. Evaluate on query set, backpropagate meta-loss, update meta-parameters

---

## Usage Example

### Basic Training
```python
from algorithms.taml import train_taml
from utils.load_omniglot import load_omniglot_dataset, OmniglotTaskDataset
from torch.utils.data import DataLoader

# Load dataset
train_dataset, test_dataset = load_omniglot_dataset(root='omniglot')

# Create task dataset
train_task_dataset = OmniglotTaskDataset(
    train_dataset,
    n_way=5,
    k_shot=1,
    num_query=15,
    num_tasks=2000
)

# Create dataloader
train_loader = DataLoader(
    train_task_dataset,
    batch_size=4,
    shuffle=True
)

# Train TAML
model, losses = train_taml(
    num_classes=5,
    task_dataloader=train_loader,
    latent_dim=64,
    modulation_approach='per_layer_scalars',
    inner_steps=5,
    outer_lr=0.001
)
```

### Manual Training Loop
```python
from algorithms.taml import TaskAgnosticMetaLearning
import torch

taml = TaskAgnosticMetaLearning(
    num_classes=5,
    latent_dim=64,
    modulation_approach='per_layer_scalars',
    inner_steps=5,
    outer_lr=0.001
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
taml = taml.to(device)
taml.initialize_optimizer(device=device)

for task_batch in train_loader:
    support_data, support_labels, query_data, query_labels = task_batch
    support_data = support_data.to(device)
    support_labels = support_labels.to(device)
    query_data = query_data.to(device)
    query_labels = query_labels.to(device)
    loss = taml.meta_train_step(
        support_data,
        support_labels,
        query_data,
        query_labels
    )
    print(f"Loss: {loss:.4f}")
```

### Adaptation to New Tasks
```python
taml.eval()
support_data, support_labels, query_data, query_labels = next(iter(test_loader))
support_data = support_data[0].to(device)
support_labels = support_labels[0].to(device)
query_data = query_data[0].to(device)
query_labels = query_labels[0].to(device)

# Encode support set
latent_codes = taml.encode_task(support_data, support_labels)

# Decode to modulations and learning rates
modulations, learning_rates = taml.decode_to_modulations(latent_codes)

# Inner loop adaptation
fast_weights = taml.inner_update(
    support_data, support_labels, modulations, learning_rates
)

# Evaluate on query set
query_logits = taml.classifier(query_data, head_weights=fast_weights)
predictions = query_logits.argmax(dim=1)
accuracy = (predictions == query_labels).float().mean()
print(f"Query accuracy: {accuracy*100:.2f}%")
```

---

## Hyperparameter Guidelines
- **latent_dim**: 32-128 (default: 64)
- **modulation_approach**: 'per_layer_scalars' (recommended), 'global_scalars' (simpler)
- **inner_steps**: 3-10 (default: 5)
- **outer_lr**: 0.0001-0.01 (default: 0.001)

---

## Advantages of TAML
1. **Task-agnostic adaptation**: Prevents overfitting to meta-training tasks
2. **Efficient inner loop**: Only adapts head parameters, not the full network
3. **Relation network**: Refines class codes using cross-class relationships
4. **Flexible modulations**: Decoder can output global or per-layer scaling factors
5. **Shared feature extractor**: CNN backbone is meta-learned and shared

---

## Comparison with LEO
| Aspect | LEO | TAML |
|--------|-----|------|
| Optimization Space | Latent space (task-specific) | Latent space (task-agnostic) |
| Adaptation | Decoder generates FC layer | Decoder modulates/adapts head |
| Relation Network | Refines latent codes | Refines latent codes |
| Bias Prevention | Task-conditional | Task-agnostic regularization |
| Generalization | Excellent for few-shot | Excellent for broad tasks |

---

## Expected Performance (Omniglot 5-way 1-shot)
- **Training time**: ~15-25 minutes (2000 tasks, GPU)
- **Expected accuracy**: 94-97%
- **Memory usage**: ~4-6 GB GPU

---

## Files
- `algorithms/taml.py`: Core TAML implementation
- `examples/taml_on_omniglot.ipynb`: Training and evaluation example
- `docs/TAML.md`: This documentation

---

## References
1. Lee, J., et al. (2019). "Task-Agnostic Meta-Learning for Few-shot Learning." ICLR 2019.
   - Paper: https://arxiv.org/abs/1905.03684
2. Rusu, A. A., et al. (2019). "Meta-Learning with Latent Embedding Optimization." ICLR 2019.
   - Paper: https://arxiv.org/abs/1807.05960

---

## Troubleshooting
- **NaN Loss**: Lower learning rates, use gradient clipping
- **Low Accuracy**: Increase latent_dim, inner_steps, train longer
- **Out of Memory**: Reduce batch_size, latent_dim, inner_steps
- **Slow Training**: Reduce inner_steps, batch_size, use GPU

---

## Future Improvements
1. **Per-parameter modulations**: More granular adaptation
2. **Attention mechanisms**: Weight examples in support set
3. **Task conditioning**: Condition decoder on task info
4. **Progressive adaptation**: Vary learning rates per step
5. **Mixture of decoders**: For different task types
